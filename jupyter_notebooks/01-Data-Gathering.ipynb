{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Data Gathering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Download Data from Kaggle and prep the data for cleaning and testing\n",
        "* Split the Data into Train, Test and Validation sets\n",
        "* Remove Bad data, if any\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* Kaggle JSON File, The authenification key\n",
        "* Kaggle API - Used to download the data\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Train, test and validation sets in inputs/datasets/cherry_leaves\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting up the Enviroment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install the Requirments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": [
          "outputPrepend"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "=2.0.0 (from scikit-learn==0.24.2->-r requirements.txt (line 7))\n  Obtaining dependency information for threadpoolctl>=2.0.0 from https://files.pythonhosted.org/packages/81/12/fd4dea011af9d69e1cad05c75f3f7202cdcbeac9b712eea58ca779a72865/threadpoolctl-3.2.0-py3-none-any.whl.metadata\n  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\nCollecting absl-py~=0.10 (from tensorflow-cpu==2.6.0->-r requirements.txt (line 8))\n  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting astunparse~=1.6.3 (from tensorflow-cpu==2.6.0->-r requirements.txt (line 8))\n  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\nCollecting clang~=5.0 (from tensorflow-cpu==2.6.0->-r requirements.txt (line 8))\n  Downloading clang-5.0.tar.gz (30 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting flatbuffers~=1.12.0 (from tensorflow-cpu==2.6.0->-r requirements.txt (line 8))\n  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\nCollecting google-pasta~=0.2 (from tensorflow-cpu==2.6.0->-r requirements.txt (line 8))\n  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting h5py~=3.1.0 (from tensorflow-cpu==2.6.0->-r requirements.txt (line 8))\n  Downloading h5py-3.1.0-cp38-cp38-manylinux1_x86_64.whl (4.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting keras-preprocessing~=1.1.2 (from tensorflow-cpu==2.6.0->-r requirements.txt (line 8))\n  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting opt-einsum~=3.3.0 (from tensorflow-cpu==2.6.0->-r requirements.txt (line 8))\n  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting six (from plotly==4.12.0->-r requirements.txt (line 5))\n  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\nCollecting termcolor~=1.1.0 (from tensorflow-cpu==2.6.0->-r requirements.txt (line 8))\n  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting typing-extensions~=3.7.4 (from tensorflow-cpu==2.6.0->-r requirements.txt (line 8))\n  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\nRequirement already satisfied: wheel~=0.35 in /home/codeany/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from tensorflow-cpu==2.6.0->-r requirements.txt (line 8)) (0.40.0)\nCollecting wrapt~=1.12.1 (from tensorflow-cpu==2.6.0->-r requirements.txt (line 8))\n  Downloading wrapt-1.12.1.tar.gz (27 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting gast==0.4.0 (from tensorflow-cpu==2.6.0->-r requirements.txt (line 8))\n  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\nCollecting tensorboard~=2.6 (from tensorflow-cpu==2.6.0->-r requirements.txt (line 8))\n  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting tensorflow-estimator~=2.6 (from tensorflow-cpu==2.6.0->-r requirements.txt (line 8))\n  Obtaining dependency information for tensorflow-estimator~=2.6 from https://files.pythonhosted.org/packages/72/5c/c318268d96791c6222ad7df1651bbd1b2409139afeb6f468c0f327177016/tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata\n  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)\nCollecting grpcio<2.0,>=1.37.0 (from tensorflow-cpu==2.6.0->-r requirements.txt (line 8))\n  Obtaining dependency information for grpcio<2.0,>=1.37.0 from https://files.pythonhosted.org/packages/11/7f/947d3931463d97e767e761f05ae3598442dd22ab9756bf99291e5e37e698/grpcio-1.56.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading grpcio-1.56.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nRequirement already satisfied: jinja2 in /home/codeany/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from altair>=3.2.0->streamlit==0.85.0->-r requirements.txt (line 6)) (3.1.2)\nRequirement already satisfied: jsonschema>=3.0 in /home/codeany/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from altair>=3.2.0->streamlit==0.85.0->-r requirements.txt (line 6)) (4.18.3)\nCollecting toolz (from altair>=3.2.0->streamlit==0.85.0->-r requirements.txt (line 6))\n  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hINFO: pip is looking at multiple versions of altair to determine which version is compatible with other requirements. This could take a while.\nCollecting altair>=3.2.0 (from streamlit==0.85.0->-r requirements.txt (line 6))\n  Downloading altair-5.0.0-py3-none-any.whl (477 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m477.4/477.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading altair-4.2.2-py3-none-any.whl (813 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m813.6/813.6 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting entrypoints (from altair>=3.2.0->streamlit==0.85.0->-r requirements.txt (line 6))\n  Downloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\nINFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\nCollecting scipy>=1.0 (from seaborn==0.11.0->-r requirements.txt (line 4))\n  Downloading scipy-1.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading scipy-1.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting google-auth<3,>=1.6.3 (from tensorboard~=2.6->tensorflow-cpu==2.6.0->-r requirements.txt (line 8))\n  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/9c/8d/bff87fc722553a5691d8514da5523c23547f3894189ba03b57592e37bdc2/google_auth-2.22.0-py2.py3-none-any.whl.metadata\n  Downloading google_auth-2.22.0-py2.py3-none-any.whl.metadata (4.2 kB)\nCollecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard~=2.6->tensorflow-cpu==2.6.0->-r requirements.txt (line 8))\n  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\nCollecting markdown>=2.6.8 (from tensorboard~=2.6->tensorflow-cpu==2.6.0->-r requirements.txt (line 8))\n  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /home/codeany/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow-cpu==2.6.0->-r requirements.txt (line 8)) (68.0.0)\nCollecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.6->tensorflow-cpu==2.6.0->-r requirements.txt (line 8))\n  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/02/52/fb9e51fba47951aabd7a6b25e41d73eae94208ccf62d886168096941a781/tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata\n  Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (1.1 kB)\nCollecting werkzeug>=1.0.1 (from tensorboard~=2.6->tensorflow-cpu==2.6.0->-r requirements.txt (line 8))\n  Obtaining dependency information for werkzeug>=1.0.1 from https://files.pythonhosted.org/packages/ba/d6/8040faecaba2feb84e1647af174b3243c9b90c163c7ea407820839931efe/Werkzeug-2.3.6-py3-none-any.whl.metadata\n  Downloading Werkzeug-2.3.6-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /home/codeany/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from requests->streamlit==0.85.0->-r requirements.txt (line 6)) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /home/codeany/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from requests->streamlit==0.85.0->-r requirements.txt (line 6)) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /home/codeany/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from requests->streamlit==0.85.0->-r requirements.txt (line 6)) (2.0.3)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /home/codeany/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from gitpython->streamlit==0.85.0->-r requirements.txt (line 6)) (4.0.10)\nCollecting backports.zoneinfo (from tzlocal->streamlit==0.85.0->-r requirements.txt (line 6))\n  Downloading backports.zoneinfo-0.2.1-cp38-cp38-manylinux1_x86_64.whl (74 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: decorator>=3.4.0 in /home/codeany/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from validators->streamlit==0.85.0->-r requirements.txt (line 6)) (5.1.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /home/codeany/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->gitpython->streamlit==0.85.0->-r requirements.txt (line 6)) (5.0.0)\nCollecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow-cpu==2.6.0->-r requirements.txt (line 8))\n  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow-cpu==2.6.0->-r requirements.txt (line 8))\n  Downloading rsa-4.9-py3-none-any.whl (34 kB)\nCollecting urllib3<3,>=1.21.1 (from requests->streamlit==0.85.0->-r requirements.txt (line 6))\n  Obtaining dependency information for urllib3<3,>=1.21.1 from https://files.pythonhosted.org/packages/c5/05/c214b32d21c0b465506f95c4f28ccbcba15022e000b043b72b3df7728471/urllib3-1.26.16-py2.py3-none-any.whl.metadata\n  Downloading urllib3-1.26.16-py2.py3-none-any.whl.metadata (48 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard~=2.6->tensorflow-cpu==2.6.0->-r requirements.txt (line 8))\n  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /home/codeany/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from jinja2->altair>=3.2.0->streamlit==0.85.0->-r requirements.txt (line 6)) (2.1.3)\nRequirement already satisfied: importlib-resources>=1.4.0 in /home/codeany/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==0.85.0->-r requirements.txt (line 6)) (6.0.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/codeany/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==0.85.0->-r requirements.txt (line 6)) (2023.6.1)\nRequirement already satisfied: pkgutil-resolve-name>=1.3.10 in /home/codeany/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==0.85.0->-r requirements.txt (line 6)) (1.3.10)\nRequirement already satisfied: referencing>=0.28.4 in /home/codeany/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==0.85.0->-r requirements.txt (line 6)) (0.29.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /home/codeany/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==0.85.0->-r requirements.txt (line 6)) (0.8.10)\nRequirement already satisfied: importlib-metadata>=4.4 in /home/codeany/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow-cpu==2.6.0->-r requirements.txt (line 8)) (6.8.0)\nRequirement already satisfied: zipp>=0.5 in /home/codeany/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow-cpu==2.6.0->-r requirements.txt (line 8)) (3.16.2)\nCollecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow-cpu==2.6.0->-r requirements.txt (line 8))\n  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard~=2.6->tensorflow-cpu==2.6.0->-r requirements.txt (line 8))\n  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\nDownloading grpcio-1.56.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading joblib-1.3.1-py3-none-any.whl (301 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading Pillow-10.0.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyparsing-3.1.0-py3-none-any.whl (102 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\nDownloading pyarrow-12.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.0/39.0 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tzlocal-5.0.1-py3-none-any.whl (20 kB)\nDownloading google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.8/181.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading Werkzeug-2.3.6-py3-none-any.whl (242 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: clang, termcolor, wrapt, validators\n  Building wheel for clang (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30682 sha256=606e77bb09d5824f7d7b2a7e8c203de55094b1494f89a3ed2b895ce0823f5c44\n  Stored in directory: /home/codeany/.cache/pip/wheels/f1/60/77/22b9b5887bd47801796a856f47650d9789c74dc3161a26d608\n  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4832 sha256=dee0b9661d3c9f353e6c2c9b19f504c34c2517d0eb941e79eb95a8738e697b2f\n  Stored in directory: /home/codeany/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp38-cp38-linux_x86_64.whl size=75894 sha256=17f4ef5d8cbca8896c4ca5c7e45d675ce3b3c2fd301283b4da53e099fce05f52\n  Stored in directory: /home/codeany/.cache/pip/wheels/5f/fd/9e/b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n  Building wheel for validators (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19579 sha256=34f41d7ec3f4bd16db654e3f82f0f839a4b29b388d709b3904ca4f0dc6625a20\n  Stored in directory: /home/codeany/.cache/pip/wheels/19/09/72/3eb74d236bb48bd0f3c6c3c83e4e0c5bbfcbcad7c6c3539db8\nSuccessfully built clang termcolor wrapt validators\nInstalling collected packages: wrapt, typing-extensions, termcolor, pytz, keras, flatbuffers, clang, werkzeug, watchdog, validators, urllib3, toolz, toml, threadpoolctl, tensorflow-estimator, tensorboard-data-server, six, pyparsing, pyasn1, protobuf, pillow, oauthlib, numpy, kiwisolver, joblib, grpcio, gast, entrypoints, cycler, click, cachetools, blinker, base58, backports.zoneinfo, astor, tzlocal, scipy, rsa, retrying, pydeck, pyasn1-modules, pyarrow, opt-einsum, markdown, keras-preprocessing, h5py, google-pasta, astunparse, absl-py, scikit-learn, requests-oauthlib, plotly, pandas, matplotlib, google-auth, seaborn, google-auth-oauthlib, altair, tensorboard, streamlit, tensorflow-cpu\n  Attempting uninstall: wrapt\n    Found existing installation: wrapt 1.15.0\n    Uninstalling wrapt-1.15.0:\n      Successfully uninstalled wrapt-1.15.0\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.7.1\n    Uninstalling typing_extensions-4.7.1:\n      Successfully uninstalled typing_extensions-4.7.1\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 2.0.3\n    Uninstalling urllib3-2.0.3:\n      Successfully uninstalled urllib3-2.0.3\n  Attempting uninstall: six\n    Found existing installation: six 1.16.0\n    Uninstalling six-1.16.0:\n      Successfully uninstalled six-1.16.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nastroid 2.15.6 requires typing-extensions>=4.0.0; python_version < \"3.11\", but you have typing-extensions 3.7.4.3 which is incompatible.\nmypy 1.4.1 requires typing-extensions>=4.1.0, but you have typing-extensions 3.7.4.3 which is incompatible.\npylint 2.17.4 requires typing-extensions>=3.10.0; python_version < \"3.10\", but you have typing-extensions 3.7.4.3 which is incompatible.\nrich 13.4.2 requires typing-extensions<5.0,>=4.0.0; python_version < \"3.9\", but you have typing-extensions 3.7.4.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed absl-py-0.15.0 altair-4.2.2 astor-0.8.1 astunparse-1.6.3 backports.zoneinfo-0.2.1 base58-2.1.1 blinker-1.6.2 cachetools-5.3.1 clang-5.0 click-7.1.2 cycler-0.11.0 entrypoints-0.4 flatbuffers-1.12 gast-0.4.0 google-auth-2.22.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.56.0 h5py-3.1.0 joblib-1.3.1 keras-2.6.0 keras-preprocessing-1.1.2 kiwisolver-1.4.4 markdown-3.4.3 matplotlib-3.3.1 numpy-1.19.2 oauthlib-3.2.2 opt-einsum-3.3.0 pandas-1.1.2 pillow-10.0.0 plotly-4.12.0 protobuf-3.20.0 pyarrow-12.0.1 pyasn1-0.5.0 pyasn1-modules-0.3.0 pydeck-0.8.1b0 pyparsing-3.1.0 pytz-2023.3 requests-oauthlib-1.3.1 retrying-1.3.4 rsa-4.9 scikit-learn-0.24.2 scipy-1.9.3 seaborn-0.11.0 six-1.15.0 streamlit-0.85.0 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-cpu-2.6.0 tensorflow-estimator-2.13.0 termcolor-1.1.0 threadpoolctl-3.2.0 toml-0.10.2 toolz-0.12.0 typing-extensions-3.7.4.3 tzlocal-5.0.1 urllib3-1.26.16 validators-0.20.0 watchdog-3.0.0 werkzeug-2.3.6 wrapt-1.12.1\n"
        }
      ],
      "source": [
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "'/workspaces/Mildew-Detection'"
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "You set a new current directory\n"
        }
      ],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "'/workspaces/Mildew-Detection'"
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "## Install Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting kaggle\n  Downloading kaggle-1.5.15.tar.gz (77 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: six>=1.10 in /home/codeany/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from kaggle) (1.15.0)\nRequirement already satisfied: certifi in /home/codeany/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from kaggle) (2023.5.7)\nRequirement already satisfied: python-dateutil in /home/codeany/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from kaggle) (2.8.2)\nRequirement already satisfied: requests in /home/codeany/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from kaggle) (2.31.0)\nCollecting tqdm (from kaggle)\n  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting python-slugify (from kaggle)\n  Downloading python_slugify-8.0.1-py2.py3-none-any.whl (9.7 kB)\nRequirement already satisfied: urllib3 in /home/codeany/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from kaggle) (1.26.16)\nRequirement already satisfied: bleach in /home/codeany/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from kaggle) (6.0.0)\nRequirement already satisfied: webencodings in /home/codeany/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from bleach->kaggle) (0.5.1)\nCollecting text-unidecode>=1.3 (from python-slugify->kaggle)\n  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /home/codeany/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from requests->kaggle) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /home/codeany/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from requests->kaggle) (3.4)\nBuilding wheels for collected packages: kaggle\n  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.5.15-py3-none-any.whl size=99605 sha256=54d64c92777d6042f64faf8561b0d25a409d4ddd13e00345b16681b866e103ef\n  Stored in directory: /home/codeany/.cache/pip/wheels/6d/41/19/71e3189da3afee54030f6aa5748f0830605f6538ba348c75b8\nSuccessfully built kaggle\nInstalling collected packages: text-unidecode, tqdm, python-slugify, kaggle\nSuccessfully installed kaggle-1.5.15 python-slugify-8.0.1 text-unidecode-1.3 tqdm-4.65.0\n"
        }
      ],
      "source": [
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set kaggle configuration to working directory, set permission for kaggle.json file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "## Download Data from Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set Kaggle Dataset and download directorys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "KaggleDatasetPath = \"codeinstitute/cherry-leaves\"\n",
        "DestinationFolder = \"inputs/cherry-leaves\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check for non image files and remove them if any"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_non_img(my_data_dir):\n",
        "    '''\n",
        "    Check the dataset for any bad data and remove it\n",
        "    '''\n",
        "    image_extensions = ('.png', '.jpeg', '.jpg')\n",
        "    for root, dirs, files in os.walk(my_data_dir):\n",
        "        # os.walk used to traverse directory structure and search for image files\n",
        "        a = 0\n",
        "        b = 0\n",
        "        for file in files:\n",
        "            if not file.lower().endswith(image_extensions):\n",
        "                file_location = os.path.join(\n",
        "                    root, file\n",
        "                ) \n",
        "                os.remove(file_location) \n",
        "                a += 1\n",
        "            else:\n",
        "                b += 1\n",
        "        print(f\"Folder: {root} has correctly formatted image files {a}\")\n",
        "        print(f\"Folder: {root} has correctly formatted image files {b}\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I'm using the os.walk method because it is more time and space efficient. It avoids having to load the entire directory at once. It generates the directory as needed.\n",
        "\n",
        "It also avoids the need to call os.path.isdir() to check where a file is. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "remove_non_img(my_data_dir='inputs/cherry_leaves')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spliting data into its datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Industry standard pushes toward a 70-10-20 split with datasets this large. \n",
        "\n",
        "70% being used for training is a good starting point, the 10% for validation will provide enough data to optimize the hyperparams without major risk of overfitting. Lastly, a 20% test set will provide a good estimate of the models preformance. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "import random\n",
        "import joblib\n",
        "\n",
        "def split_train_validation_test_images(my_data_dir, train_set_ratio, validation_set_ratio, test_set_ratio):\n",
        "    \"\"\"\n",
        "    split data set into three groups by ratio's .7, .1, .2\n",
        "    \"\"\"\n",
        "    if train_set_ratio + validation_set_ratio + test_set_ratio != 1.0:\n",
        "        print(\"train_set_ratio + validation_set_ratio + test_set_ratio should sum to 1.0\")\n",
        "        return\n",
        "\n",
        "    # gets classes labels\n",
        "    labels = os.listdir(my_data_dir)  # it should get only the folder name\n",
        "    if 'test' in labels:\n",
        "        pass\n",
        "    else:\n",
        "        # create train, test folders with classes labels sub-folder\n",
        "        for folder in ['train', 'validation', 'test']:\n",
        "            for label in labels:\n",
        "                os.makedirs(name=my_data_dir + '/' + folder + '/' + label)\n",
        "\n",
        "        for label in labels:\n",
        "\n",
        "            files = os.listdir(my_data_dir + '/' + label)\n",
        "            random.shuffle(files)\n",
        "\n",
        "            train_set_files_qty = int(len(files) * train_set_ratio)\n",
        "            validation_set_files_qty = int(len(files) * validation_set_ratio)\n",
        "\n",
        "            # Move files to appropriate set directories\n",
        "            # Use of enumerate leads to improved memory efficiency and faster execution time,\n",
        "            # particularly in cases where the loop is iterating over a large number of items.\n",
        "            for count, file_name in enumerate(files):\n",
        "                if count < train_set_files_qty:\n",
        "                    # move a given file to the train set\n",
        "                    shutil.move(my_data_dir + '/' + label + '/' + file_name,\n",
        "                                my_data_dir + '/train/' + label + '/' + file_name)\n",
        "\n",
        "                elif count < (train_set_files_qty + validation_set_files_qty):\n",
        "                    # move a given file to the validation set\n",
        "                    shutil.move(my_data_dir + '/' + label + '/' + file_name,\n",
        "                                my_data_dir + '/validation/' + label + '/' + file_name)\n",
        "\n",
        "                else:\n",
        "                    # move given file to test set\n",
        "                    shutil.move(my_data_dir + '/' + label + '/' + file_name,\n",
        "                                my_data_dir + '/test/' + label + '/' + file_name)\n",
        "\n",
        "            os.rmdir(my_data_dir + '/' + label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'inputs/cherry_leaves_dataset/cherry-leaves'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msplit_train_validation_test_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_data_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs/cherry_leaves_dataset/cherry-leaves\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mtrain_set_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mvalidation_set_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mtest_set_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[15], line 14\u001b[0m, in \u001b[0;36msplit_train_validation_test_images\u001b[0;34m(my_data_dir, train_set_ratio, validation_set_ratio, test_set_ratio)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# gets classes labels\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_data_dir\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# it should get only the folder name\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m labels:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'inputs/cherry_leaves_dataset/cherry-leaves'"
          ]
        }
      ],
      "source": [
        "split_train_validation_test_images(my_data_dir=f\"inputs/cherry_leaves_dataset/cherry-leaves\",\n",
        "                                   train_set_ratio=0.7,\n",
        "                                   validation_set_ratio=0.1,\n",
        "                                   test_set_ratio=0.2\n",
        "                                   )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Push files to Github"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "git add .\n",
        "\n",
        "git commit -m \"Add and prepare cherry leaves dataset\"\n",
        "\n",
        "git push"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[02-Visualisation](jupyter_notebooks/02-Visualisation.ipynb)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "language": "python",
      "name": "python381264bit3812pyenv03bc028c58684e3e8a3af308acfbdc5e"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12-final"
    },
    "orig_nbformat": 2,
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}